{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10304280,"sourceType":"datasetVersion","datasetId":6378370}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport pandas as pd \nimport re\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_path = \"/kaggle/input/piqa-eval/piqa_validation.json\"\n\nval_df = pd.read_json(val_path)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prompt","metadata":{}},{"cell_type":"code","source":"prompt= \"\"\"You are an intelligent assistant that helps evaluate practical solutions to everyday tasks and problems. Your role is to analyze solutions in a given context and determine which one is more effective and practical.\n\nGiven a context and two possible solutions, choose the better solution that makes more sense for that context.\n\nHere are some important rules for the task:\n- Think carefully and logically about which solution is more appropriate for the given context\n- Consider practicality, safety, and effectiveness when evaluating the solutions\n- You must explain your reasoning for why your chosen solution is better\n- You must answer with <answer>0</answer> if Solution 1 is better, or <answer>1</answer> if Solution 2 is better\n- Put your final answer in <answer></answer> tags after your explanation\n\nHere are some examples:\n<example>\n\n<question>\nBased on the context, which solution is better?\nContext: When boiling butter, when it's ready, you can\nSolution 1: Pour it onto a plate\nSolution 2: Pour it into a jar\n</question>\n\n<response>\nSolution 1 is better because it's easier to clean up and it's more practical.\nFinal answer: <answer>0</answer>\n</response>\n\n</example>\n\nNow, consider the following context and solutions, choose the better solution that makes more sense for that context.\nContext: {goal}\n\nSolution 1: {sol1}\nSolution 2: {sol2}\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gemini","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Log in to Hugging Face\nlogin()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n# Move model to GPU if available\nif torch.cuda.is_available():\n    device = [0, 1]  # Use GPUs 0 and 1\n    model = torch.nn.DataParallel(model, device_ids=device)\n# model = model.to(\"cuda:0\")\nelse:\n    device = \"cpu\"\n    model = model.to(device)\n\ndef post_process(response):\n    pattern = r'<answer>(.*?)</answer>'\n    match = re.search(pattern, response)\n    if match:\n        return match.group(1)\n    return None\n\n\ndef get_llama_response(row):\n    # Format the prompt with the current example\n    formatted_prompt = prompt.format(\n        goal=row['goal'],\n        sol1=row['sol1'], \n        sol2=row['sol2']\n    )\n    \n    # Tokenize and generate\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device if isinstance(device, str) else f'cuda:{device[0]}')\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=128,\n        temperature=0.7,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    # Decode and return response\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nwith torch.no_grad():\n    res = []\n    cnt = 0\n    for idx, row in val_df.iterrows():\n        response = get_llama_response(row)\n        final_answer = post_process(response)\n        res.append(final_answer) \n        cnt += 1\n        if cnt == 5: break \n    \nval_df['llama_answer'] = res\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_df.to_csv(\"llama_eval.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy = (val_df['llama_answer'] == val_df['label']).mean()\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}