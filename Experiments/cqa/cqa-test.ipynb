{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T00:06:34.027888Z",
     "iopub.status.busy": "2024-12-29T00:06:34.027564Z",
     "iopub.status.idle": "2024-12-29T00:06:34.031902Z",
     "shell.execute_reply": "2024-12-29T00:06:34.031013Z",
     "shell.execute_reply.started": "2024-12-29T00:06:34.027864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T00:06:34.789563Z",
     "iopub.status.busy": "2024-12-29T00:06:34.789272Z",
     "iopub.status.idle": "2024-12-29T00:06:41.185996Z",
     "shell.execute_reply": "2024-12-29T00:06:41.184883Z",
     "shell.execute_reply.started": "2024-12-29T00:06:34.789542Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:57:39.439057Z",
     "iopub.status.busy": "2024-12-28T23:57:39.438810Z",
     "iopub.status.idle": "2024-12-28T23:57:39.443154Z",
     "shell.execute_reply": "2024-12-28T23:57:39.442179Z",
     "shell.execute_reply.started": "2024-12-28T23:57:39.439036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:57:39.444768Z",
     "iopub.status.busy": "2024-12-28T23:57:39.444488Z",
     "iopub.status.idle": "2024-12-28T23:57:39.485281Z",
     "shell.execute_reply": "2024-12-28T23:57:39.484614Z",
     "shell.execute_reply.started": "2024-12-28T23:57:39.444747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_path = \"/kaggle/input/cqa-dataset/dev_rand_split.jsonl\"\n",
    "\n",
    "# Load the JSONL file as a DataFrame\n",
    "val_df = pd.read_json(val_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:57:39.486242Z",
     "iopub.status.busy": "2024-12-28T23:57:39.486023Z",
     "iopub.status.idle": "2024-12-28T23:57:39.504579Z",
     "shell.execute_reply": "2024-12-28T23:57:39.503900Z",
     "shell.execute_reply.started": "2024-12-28T23:57:39.486224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answerKey</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1afa02df02c908a558b4036e80242fac</td>\n",
       "      <td>{'question_concept': 'revolving door', 'choice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>a7ab086045575bb497933726e4e6ad28</td>\n",
       "      <td>{'question_concept': 'people', 'choices': [{'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>b8c0a4703079cf661d7261a60a1bcbff</td>\n",
       "      <td>{'question_concept': 'magazines', 'choices': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>e68fb2448fd74e402aae9982aa76e527</td>\n",
       "      <td>{'question_concept': 'hamburger', 'choices': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2435de612dd69f2012b9e40d6af4ce38</td>\n",
       "      <td>{'question_concept': 'farmland', 'choices': [{...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  answerKey                                id  \\\n",
       "0         A  1afa02df02c908a558b4036e80242fac   \n",
       "1         A  a7ab086045575bb497933726e4e6ad28   \n",
       "2         B  b8c0a4703079cf661d7261a60a1bcbff   \n",
       "3         A  e68fb2448fd74e402aae9982aa76e527   \n",
       "4         A  2435de612dd69f2012b9e40d6af4ce38   \n",
       "\n",
       "                                            question  \n",
       "0  {'question_concept': 'revolving door', 'choice...  \n",
       "1  {'question_concept': 'people', 'choices': [{'l...  \n",
       "2  {'question_concept': 'magazines', 'choices': [...  \n",
       "3  {'question_concept': 'hamburger', 'choices': [...  \n",
       "4  {'question_concept': 'farmland', 'choices': [{...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:57:39.505670Z",
     "iopub.status.busy": "2024-12-28T23:57:39.505450Z",
     "iopub.status.idle": "2024-12-28T23:57:39.510679Z",
     "shell.execute_reply": "2024-12-28T23:57:39.510017Z",
     "shell.execute_reply.started": "2024-12-28T23:57:39.505651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_concept': 'people',\n",
       " 'choices': [{'label': 'A', 'text': 'complete job'},\n",
       "  {'label': 'B', 'text': 'learn from each other'},\n",
       "  {'label': 'C', 'text': 'kill animals'},\n",
       "  {'label': 'D', 'text': 'wear hats'},\n",
       "  {'label': 'E', 'text': 'talk to each other'}],\n",
       " 'stem': 'What do people aim to do at work?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['question'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:57:39.511534Z",
     "iopub.status.busy": "2024-12-28T23:57:39.511349Z",
     "iopub.status.idle": "2024-12-28T23:57:39.523767Z",
     "shell.execute_reply": "2024-12-28T23:57:39.523101Z",
     "shell.execute_reply.started": "2024-12-28T23:57:39.511516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a highly intelligent assistant specializing in solving multiple-choice commonsense reasoning questions. Your task is select the most logical and contextually accurate answer.\n",
    "\n",
    "### Guidelines for the Task:\n",
    "**Format the Final Answer**:  Format your final answer as `<answer>D</answer>` where \"D\" is the correct option letter.\n",
    "\n",
    "### Task Input:\n",
    "**Question ID**: {id}  \n",
    "**Concept**: {question_concept}  \n",
    "**Stem**: {stem}  \n",
    "**Choices**:  \n",
    "{formatted_choices}\n",
    "\n",
    "### Task Output:\n",
    "**Final Answer**: Format your final answer as `<answer>D</answer>` where \"D\" is the correct option letter.\n",
    "\n",
    "Proceed to evaluate the following question:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2.5 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:12:37.556161Z",
     "iopub.status.busy": "2024-12-28T23:12:37.555879Z",
     "iopub.status.idle": "2024-12-28T23:12:37.811610Z",
     "shell.execute_reply": "2024-12-28T23:12:37.810724Z",
     "shell.execute_reply.started": "2024-12-28T23:12:37.556140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"key_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:12:38.596237Z",
     "iopub.status.busy": "2024-12-28T23:12:38.595924Z",
     "iopub.status.idle": "2024-12-28T23:12:39.256618Z",
     "shell.execute_reply": "2024-12-28T23:12:39.255898Z",
     "shell.execute_reply.started": "2024-12-28T23:12:38.596213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face\n",
    "\n",
    "login(secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T16:16:39.518024Z",
     "iopub.status.busy": "2024-12-28T16:16:39.517705Z",
     "iopub.status.idle": "2024-12-28T16:16:54.873285Z",
     "shell.execute_reply": "2024-12-28T16:16:54.872580Z",
     "shell.execute_reply.started": "2024-12-28T16:16:39.518000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcf51821d2545a8a048e3a95b3c1b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "# Configure the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Helper functions\n",
    "def post_process(response):\n",
    "    import re\n",
    "    pattern = r'<answer>(.*?)</answer>'\n",
    "    matches = re.findall(pattern, response)\n",
    "    if matches:\n",
    "        return matches[-1]  # Lấy phần tử cuối\n",
    "        \n",
    "    return None\n",
    "\n",
    "def get_qwen_response(row):\n",
    "    # Define the prompt format (ensure this is set correctly)\n",
    "    formatted_prompt = prompt.format(\n",
    "        id=row['id'],\n",
    "        question_concept=row['question']['question_concept'],\n",
    "        stem=row['question']['stem'],\n",
    "        formatted_choices=\"\\n\".join(\n",
    "            [f\"{choice['label']}. {choice['text']}\" for choice in row['question']['choices']]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T16:16:54.874471Z",
     "iopub.status.busy": "2024-12-28T16:16:54.873992Z",
     "iopub.status.idle": "2024-12-28T16:16:54.880092Z",
     "shell.execute_reply": "2024-12-28T16:16:54.879300Z",
     "shell.execute_reply.started": "2024-12-28T16:16:54.874446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_concept': 'people',\n",
       " 'choices': [{'label': 'A', 'text': 'complete job'},\n",
       "  {'label': 'B', 'text': 'learn from each other'},\n",
       "  {'label': 'C', 'text': 'kill animals'},\n",
       "  {'label': 'D', 'text': 'wear hats'},\n",
       "  {'label': 'E', 'text': 'talk to each other'}],\n",
       " 'stem': 'What do people aim to do at work?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['question'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T16:16:54.881045Z",
     "iopub.status.busy": "2024-12-28T16:16:54.880849Z",
     "iopub.status.idle": "2024-12-28T16:32:30.441769Z",
     "shell.execute_reply": "2024-12-28T16:32:30.440679Z",
     "shell.execute_reply.started": "2024-12-28T16:16:54.881027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time  # Đảm bảo module time được import\n",
    "\n",
    "res = []\n",
    "for idx, row in val_df.iterrows():\n",
    "    response = get_qwen_response(row)\n",
    "    #print(response)\n",
    "    final_answer = post_process(response)\n",
    "    #print(idx,final_answer)\n",
    "    res.append(final_answer) \n",
    "    time.sleep(10)\n",
    "val_df['qwen_answer'] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T16:32:30.443005Z",
     "iopub.status.busy": "2024-12-28T16:32:30.442729Z",
     "iopub.status.idle": "2024-12-28T16:32:30.450450Z",
     "shell.execute_reply": "2024-12-28T16:32:30.449536Z",
     "shell.execute_reply.started": "2024-12-28T16:32:30.442982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_df.to_csv(\"qwen_eval.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T16:32:30.451664Z",
     "iopub.status.busy": "2024-12-28T16:32:30.451354Z",
     "iopub.status.idle": "2024-12-28T16:32:30.463691Z",
     "shell.execute_reply": "2024-12-28T16:32:30.462832Z",
     "shell.execute_reply.started": "2024-12-28T16:32:30.451635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "accuracy = (val_df['qwen_answer'] == val_df['answerKey']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T00:08:43.607939Z",
     "iopub.status.busy": "2024-12-29T00:08:43.607578Z",
     "iopub.status.idle": "2024-12-29T00:08:43.805345Z",
     "shell.execute_reply": "2024-12-29T00:08:43.804194Z",
     "shell.execute_reply.started": "2024-12-29T00:08:43.607910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Configure the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Helper functions\n",
    "def post_process(response):\n",
    "    import re\n",
    "    pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_llama_response(row):\n",
    "    # Define the prompt format (ensure this is set correctly)\n",
    "    formatted_prompt = prompt.format(\n",
    "        id=row['id'],\n",
    "        question_concept=row['question']['question_concept'],\n",
    "        stem=row['question']['stem'],\n",
    "        formatted_choices=\"\\n\".join(\n",
    "            [f\"{choice['label']}. {choice['text']}\" for choice in row['question']['choices']]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-28T16:32:30.740774Z",
     "iopub.status.idle": "2024-12-28T16:32:30.741079Z",
     "shell.execute_reply": "2024-12-28T16:32:30.740967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "res = []\n",
    "for idx, row in val_df.iterrows():\n",
    "    response = get_llama_response(row)\n",
    "    #print(response)\n",
    "    final_answer = post_process(response)\n",
    "    #print(idx,final_answer)\n",
    "    res.append(final_answer) \n",
    "\n",
    "val_df['llma_answer'] = res\n",
    "accuracy = (val_df['llma_answer'] == val_df['answerKey']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T23:57:39.525554Z",
     "iopub.status.busy": "2024-12-28T23:57:39.525332Z",
     "iopub.status.idle": "2024-12-28T23:57:41.004353Z",
     "shell.execute_reply": "2024-12-28T23:57:41.003656Z",
     "shell.execute_reply.started": "2024-12-28T23:57:39.525536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"Gemini_key\")\n",
    "genai.configure(api_key=secret_value_0)\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import re\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import time\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure the Gemini API\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"Gemini_key\")\n",
    "genai.configure(api_key=secret_value_0)\n",
    "model = genai.GenerativeModel('gemini-1.5-pro') # Or 'gemini-1.5-pro-vision' if you want vision\n",
    "\n",
    "# Post-process the response\n",
    "def post_process(response):\n",
    "    if not response:\n",
    "        return None\n",
    "    pattern = r\"<answer>(.*?)</answer>\"\n",
    "    match = re.search(pattern, response)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Generate response from Gemini with retry and exception handling\n",
    "@retry(stop=stop_after_attempt(5),\n",
    "       wait=wait_exponential(multiplier=1, min=1, max=15),\n",
    "       reraise=True) # reraise to let the outer loop handle failure after all retries\n",
    "def get_gemini_response(row):\n",
    "    formatted_prompt = prompt.format(\n",
    "        id=row['id'],\n",
    "        question_concept=row['question']['question_concept'],\n",
    "        stem=row['question']['stem'],\n",
    "        formatted_choices=\"\\n\".join(\n",
    "            [f\"{choice['label']}. {choice['text']}\" for choice in row['question']['choices']]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(formatted_prompt)\n",
    "    return response.text\n",
    "    \n",
    "\n",
    "import time\n",
    "# Process rows and collect responses\n",
    "res = []\n",
    "for idx in range(3):\n",
    "    time.sleep(20)\n",
    "    response = get_gemini_response(val_df.iloc[idx])\n",
    "    print(response)\n",
    "    final_answer = post_process(response)\n",
    "    print(final_answer)\n",
    "    res.append(final_answer)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-28T16:32:30.743912Z",
     "iopub.status.idle": "2024-12-28T16:32:30.744286Z",
     "shell.execute_reply": "2024-12-28T16:32:30.744120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add answers to DataFrame and save results\n",
    "val_df['gemini_answer'] = res\n",
    "val_df.to_csv(\"gemini_eval.csv\", index=False)\n",
    "\n",
    "# Normalize answers for accuracy calculation\n",
    "val_df['gemini_answer'] = val_df['gemini_answer'].str.strip().str.lower()\n",
    "val_df['answerKey'] = val_df['answerKey'].str.strip().str.lower()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (val_df['gemini_answer'] == val_df['answerKey']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6378370,
     "sourceId": 10304280,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6379462,
     "sourceId": 10306678,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
