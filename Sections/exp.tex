\subsection{Evaluating Casual Reasoning in Large Language Models}

\subsubsection{Introduction}
Based on the aforementioned methods for evaluating the causal reasoning abilities of large language models, in this section, we will conduct an assessment 
and examine the capabilities of modern language models in this type of reasoning through various prompting techniques.

\paragraph{Datasets} We evaluate using the COPA dataset, which is utilized for assessing causal reasoning abilities across a broad domain. 
COPA consists of 1000 questions, equally divided into two sets: a development set and a test set, each containing 500 questions (here, we will use the first 100 questions from the test set for evaluation). Each question includes a premise and two alternative choices. 
The primary task for LLMs is to provide answers A and B, such that the selected answer has a closer causal relationship to the premise.

\paragraph{Model and Prompting Techniques} To evaluate causal reasoning abilities, we conducted tests on four different models, all with a size of 7B parameters, including: Llama3-7B, Mistral-7B, Gemma-7B, and Qwen2-7B.
These are all newly released models that have achieved some impressive results in several LLM benchmarks. Simultaneously, we employed four different prompting methods: Few-shot learning + COT, Zero-shot learning + COT, Few-shot learning, and Zero-shot learning for each of the models mentioned above

\subsubsection{Results}
We performed inference using Ollama with the four models mentioned above, following the four different prompting approaches. Simultaneously, we used two metrics for evaluation: Accuracy and F1-score. This is appropriate because the task requires providing a correct result, specifically label A or label B. 
Below is a table comparing the results of each model across the different prompting methods (using 100 questions from the COPA dataset's test set).

\begin{table}[ht]
  \centering
  \begin{tabular}{|l|c|c|c|c|}
      \hline
      \textbf{Model}    & \textbf{Few-shot + COT} & \textbf{Few-shot} & \textbf{Zero-shot} & \textbf{Zero-shot + COT} \\ \hline

      Llama3-7B & 0.9, 0.87  & 0.88, 0.82  & 0.6, 0.62  & 0.73, 0.75  \\ \hline 

      Gemma2-7B & 0.94, 0.9 & 0.83, 0.86 & 0.7, 0.75 & 0.77, 0.8 \\ \hline 

      Mistral-7b & 0.95, 0.96 & 0.89, 0.91 & 0.62, 0.65 & 0.7, 0.72 \\ \hline

      Qwen2-7B & 0.97, 0.92 & 0.9, 0.86 & 0.72, 0.78 & 0.84, 0.84 \\ \hline

  \end{tabular}
  \caption{Comparison of models across different prompting methods (accuracy - left, F1-score - right)}
  \label{tab:model_comparison}
\end{table}


Based on the information provided in the table, it can be observed that most models achieve good results when using the few-shot learning method combined with COT. Conversely, using zero-shot alone does not yield high performance. 
This suggests that few-shot + CoT is an effective method for optimizing the causal reasoning ability of LLMs.Qwen2-7B demonstrates the best performance in most cases, particularly with the Few-shot + COT method (0.97). 
This indicates that it is a relatively strong model in terms of reasoning ability and well-suited for this type of prompting. Additionally, Gemma2-7B and Llama3-7B have relatively similar performance, with Gemma2-7B performing slightly better.

In summary, our experiment has yielded the following findings:

\begin{enumerate}
  \item The Few-shot + COT method is the most effective for enhancing causal reasoning abilities in LLMs.
  \item Qwen2-7B is the best-performing model in terms of causal reasoning, particularly with the Few-shot + COT method.
  \item Zero-shot learning alone is not effective for improving causal reasoning abilities.
\end{enumerate}


\subsection{Evaluating Commonsense Question and Answering in Large Language Models}

\subsubsection{Introduction}
This section focuses on assessing the ability of large language models to handle commonsense question-answering tasks. By employing various prompting techniques, we aim to evaluate and analyze their performance in understanding and reasoning about everyday scenarios and knowledge.

\paragraph{Datasets}
Our evaluation utilizes the ConceptNet dataset, a comprehensive resource for commonsense knowledge, to assess the commonsense question-answering abilities of large language models. The dataset consists of questions, each accompanied by four answer choices. The task for the models is to select the option that best aligns with the underlying commonsense knowledge required to answer the question correctly. This approach provides a focused evaluation of the models' reasoning and understanding of everyday concepts.


\paragraph{Model and Prompting Techniques} 
To evaluate causal reasoning abilities, we conducted tests on three different models, including Qwen-3B, Gemini, and Llama-3.2-3B. These models represent newly released advancements that have demonstrated impressive performance on several LLM benchmarks.

\subsubsection{Results}
The table below presents a comparison of the performance of each model across different prompting methods, evaluated using 50 questions from the test set of the ConceptNet dataset.

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Llama3-3B}    & \textbf{Gemini} & \textbf{Qwen2-3B} \\ \hline

         & 0.86 & 0.58\\ \hline 

  \end{tabular}
  \caption{Comparison of models by accuracy.}
  \label{tab:model_comparison}
\end{table}
