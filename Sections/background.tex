\subsection{Difinition of Reasoning}

Reasoning is a broad and multifaceted concept that manifests in various contexts. 
It encompasses cognitive and logical processes used to analyze information, draw 
inferences, reach conclusions, and construct coherent arguments. Reasoning can be 
observed across numerous domains such as scientific research, problem-solving, and 
decision-making. The purpose of reasoning is to enable individuals to connect pieces 
of information, evaluate relationships, and make informed judgments or solutions. 

Beyond its broad definition, the term “reasoning” also carries specific meanings across various fields:
\begin{itemize}
    \item In Philosophy (Cognitive Reasoning): Refers to modeling the human ability to draw meaningful conclusions even with incomplete and inconsistent knowledge. This includes ensuring that all processes, from acquiring and updating knowledge to drawing conclusions, are deployable and executable on appropriate hardware.
    \item In Logic (Logical Reasoning): Involves a systematic thought process where conclusions are derived methodically based on premises and their interrelations. This ensures that conclusions are logically implied or necessarily follow from the premises.
    \item In NLP (Natural Language Reasoning): Represents an integrated process of utilizing diverse knowledge sources to derive new conclusions about the world (real or hypothetical). The knowledge can be drawn from both explicit and implicit sources, with conclusions taking the form of statements or facts believed to be true in the world, or actionable decisions.
\end{itemize}

\subsection{Deductive, Abductive, and Inductive Reasoning}

From a traditional perspective, reasoning is categorized into three types: Deductive Reasoning, Abductive Reasoning, and Inductive Reasoning. This classification has long been recognized and provides a framework for understanding different modes of reasoning. By examining each type, we can gain deeper insights into their distinct characteristics and applications.

First, deductive reasoning is a logical process that derives specific conclusions from general principles or premises. This top-down approach begins with overarching principles and applies logical rules to arrive at specific conclusions. The goal is to provide logically valid and conclusive results.

Next, inductive reasoning involves drawing general conclusions or patterns based on specific observations or evidence, moving from particular instances to broader generalizations. This type of reasoning does not guarantee absolute certainty of the outcomes but provides likely conclusions based on the available evidence (Wang et al., 2023o).

Finally, abductive reasoning is the process of formulating plausible explanations or hypotheses to account for observed events or data. This type of reasoning involves inferring the most likely explanation from incomplete or limited information and is often used in problem-solving and hypothesis generation.

In addition to the classification above, reasoning also includes other classifications based on reasoning tasks, including the classifications presented below:
\begin{itemize}
    \item Formal reasoning vs Informal reasoning (Evans and Thompson, 2004; Teig and Scherer, 2016): These two reasoning tasks are classified based on the nature of the reasoning process. Specifically, formal reasoning involves following strict rules, logical principles, or formal systems to draw conclusions and is often based on mathematical or deductive reasoning. Informal reasoning is less structured and more intuitive, often relying on personal experience, common sense, and heuristic methods.
    \item Neural reasoning vs Symbolic reasoning vs  Neural-Symbolic Reasoning (Garcez et al., 2008, 2015, 2022): These two reasoning tasks are classified based on the computational framework used for reasoning. Specifically, neural reasoning refers to methods that use neural networks or deep learning models for reasoning tasks. Symbolic reasoning involves the use of symbolic representations, logic-based inference rules, or symbolic manipulation for reasoning. Additionally, we can combine these two forms of reasoning to leverage the strengths of each respective reasoning approach.
    \item Backward reasoning vs Forward reasoning (Al-Ajlan, 2015): These two reasoning tasks are classified based on the direction of the reasoning process. Specifically, backward reasoning starts with a goal or desired outcome and applies rules to identify the conditions or steps needed to achieve that goal. Conversely, forward reasoning begins with initial premises and proceeds step by step to derive new conclusions or reach a final outcome.
    \item Single-step reasoning vs Multi-step reasoning (Song et al., 2018; Yuet al., 2023a): These reasoning tasks are classified based on complexity or the number of reasoning steps. Single-step reasoning typically involves a single step, while multi-step reasoning usually involves multiple steps and processes.
    \item Deductive reasoning vs Defeasible reasoning g (Yu et al., 2023a; Koons,2005; Pollock, 1987, 1991): These reasoning tasks are classified based on the nature of the reasoning process and the handling of exceptions or conflicting information. Deductive reasoning involves drawing conclusions that are logically certain based on given premises. In contrast, defeasible reasoning deals with reasoning under uncertainty or incomplete information, where conclusions can be overridden by new evidence or exceptions.
    \item Unimodal reasoning vs Multimodal reasoning g (Sowa, 2003; Oberlander et al., 1996): These reasoning tasks are classified based on the input modalities used in the reasoning process. Unimodal reasoning involves a single modality or input, such as reasoning based solely on language. In contrast, multimodal reasoning involves integrating and reasoning across multiple modalities simultaneously, such as combining language, images, and audio.
\end{itemize}

\subsection{Foundation models and rencent progress}

In recent years, the field of artificial intelligence has seen remarkable advancements with the emergence of foundation models. These models have transformed numerous areas, such as computer vision, natural language processing, and speech recognition, among others. Below, we present the three main types of foundation models and their representative works.

\subsubsection{Language Foundation Models and Language Prompt}

Today, foundation models like GPT-3 (Brown et al., 2020) have made groundbreaking advances in understanding language and generating coherent, contextually appropriate responses in natural language. These models have achieved significant progress in various language-related tasks, including text completion, translation, dialogue, summarization, and question answering, among others.

In recent years, advancements in research and improvements in training methodologies have led to the emergence of numerous state-of-the-art large-scale language models. Notable examples include GPT-4 (OpenAI, 2023a), which powers ChatGPT, and PaLM (Chowdhery et al., 2022), a key component of Bard. Additionally, open-source models like LLaMA (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b) have gained traction, offering parameter configurations ranging from 7 billion to 65 billion. 

\subsubsection{Vision Foundation Models and Visual Prompt}

Building on the remarkable success of language models, foundation models have expanded into the vision domain, achieving notable milestones. Among these, the Vision Transformer (ViT) (Dosovitskiy et al., 2021) applies the Transformer framework to vision tasks, delivering outstanding results in image classification and retrieval by leveraging self-attention mechanisms. Swin Transformer (Liu et al., 2021b) enhances this approach with a hierarchical design and shifted windows, improving the efficiency of high-resolution image processing and excelling in tasks such as object detection, semantic segmentation, and classification.

Next is Masked modeling, which has emerged as a powerful self-supervised learning strategy for visual representations. Approaches such as MAE (He et al., 2022), BEIT (Bao et al., 2021), and CAE (Chen et al., 2023i) demonstrate its effectiveness in learning general-purpose representations. For video understanding, VideoMAE V2 (Wang et al., 2023i) extends VideoMAE (Tong et al., 2022) with a billion-parameter model, excelling in tasks such as action classification and detection by learning temporal and spatial dependencies.

Multitask vision foundation models, such as Florence (Yuan et al., 2021) and Florence-2 (Ding et al., 2022; Xiao et al., 2023a), support a wide range of tasks, including classification, object detection, retrieval, VQA, image captioning, and action recognition. Similarly, the Segment Anything Model (SAM) (Kirillov et al., 2023) offers remarkable segmentation capabilities, generating masks for objects from input prompts like points, boxes, or partial masks.

\subsubsection{Multimodal Foundation Models}

Multimodal foundation models have also achieved remarkable successes. One such model is Text2Seg (Zhang et al., 2023d), which introduces a vision-language model that uses text prompts to generate segmentation masks. It creates bounding boxes with Grounding DINO (Liu et al., 2023j), guiding SAM during the segmentation process. CLIP (Radford et al., 2021) learns joint representations of images and text, aligning visual and textual information to enable cross-modal understanding and achieving impressive results across various vision-language tasks. Similarly, methods such as ALIGN (Jia et al., 2021) and WenLan (Huo et al., 2021) align image and text representations by learning a common feature space.

CoOp (Context Optimization) (Zhou et al., 2022b) offers a simple yet effective technique to adapt CLIP-like vision-language models for downstream tasks. It uses learnable vectors to represent context words in a prompt while keeping the pre-trained parameters fixed. Another advancement, GALIP (Generative Adversarial CLIPs) (Tao et al., 2023), was specifically developed for text-to-image generation tasks. In CLIP Surgery (Li et al., 2023t), text prompts are used to generate heatmaps, from which point prompts are sampled and input into SAM (Kirillov et al., 2023) for further segmentation. A similarity algorithm employing CLIP is then used to produce the final segmentation map. SAMText (He et al., 2023) introduces a flexible method for generating segmentation masks tailored to scene text, using bounding box coordinates from existing scene text detection models as prompts for SAM.

Caption Anything (Wang et al., 2023p) presents an enhanced framework for image captioning, enabling interactive multimodal control using both visual and linguistic inputs. By integrating SAM with ChatGPT, users can manipulate images using various prompts like points or bounding boxes, while Large Language Models (LLMs) refine the instructions for better alignment with the user's intent. GPT-4V (OpenAI, 2023b) enhances the ability to interpret and analyze user-provided image inputs.

The potential of foundation models in multimodal tasks is highly promising, opening up exciting possibilities across various fields. By integrating information from different modalities, these models can enhance tasks such as image captioning, visual question answering, and audio-visual scene understanding. Furthermore, multimodal foundation models hold great promise in applications that require reasoning and decision-making from multiple sources of information. By harnessing the power of multimodal data, these models can significantly improve understanding, context awareness, and performance across a wide range of domains, including robotics (Firoozi et al., 2023), healthcare (Qiu et al., 2023a), autonomous vehicles (Zhou et al., 2023c), and multimedia analysis.