\subsection{Causal Reasoning}

The T\"{u}bingen cause-effect pairs dataset \citep{Mooij2016} encompasses 108 cause-effect pairs obtained from 37 datasets spanning diverse domains, including meteorology, biology, medicine, engineering, and economics. This dataset serves as a benchmark for evaluating causal reasoning abilities. In contrast, the Neuropathic Pain dataset \citep{Tu2019} focuses on the relationships between nerves and the corresponding symptoms observed in patients. Due to its specialized medical terminology and domain-specific knowledge, interpreting the variable names within this dataset requires expertise in the field. The Arctic sea ice dataset \citep{Huang2021c} presents a graph derived from domain knowledge, featuring 12 variables and 48 edges. It offers valuable insights into the dynamics of Arctic sea ice.

Counterfactual reasoning, even in the absence of actual causality, is a valuable capability for language models. It aids in decision-making, planning, and uncovering hidden insights that may not be immediately apparent in the original context. CRASS (Counterfactual Reasoning Assessment) \citep{Frohberg2021} is a benchmark specifically developed to evaluate the proficiency of language models in dealing with counterfactual queries. This benchmark comprises 275 instances in which the language model is presented with counterfactual conditional questions. In each instance, the model is tasked with selecting the most suitable response from a provided set of multiple-choice options.

In evaluating the performance of language models on causal reasoning benchmarks and datasets like CRASS, the commonly used metric is top-k accuracy \citep{Frohberg2021}. This metric quantifies the model's ability to make correct predictions by considering the top k ranked choices. It serves as a quantitative measure of the model's proficiency in causal reasoning tasks. Percentage of Preference \citep{Li2023g} is a metric used to evaluate logical completions in both counterfactual and factual scenarios. This metric provides a quantitative measure of the extent to which a language model's generated completions align with human preferences and judgments in terms of logical consistency