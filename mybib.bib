@article{in1,
  author = {Manning CD},
  title = {Human language understanding & reasoning. Daedalus 151(2):127–138},
  year = {2022}
}

@article{in2,
  author = {Daniel K},
  title = {Thinking, fast and slow},
  year = {2017}
}

@article{in3,
  author = {Bengio Y},
  title = {The consciousness prior. arXiv preprint},
  year = {2017}
}

@article{in4,
  author = {Weston J, Sukhbaatar S },
  title = {System 2 attention (is something you might need too)},
  year = {2023}
}

@article{in5,
  author = {Reiter R},
  title = {Formal reasoning and language understanding system. In: Theoretical
Issues in Natural Language Processing},
  year = {1975}
}


@article{in6,
  author = {Berzonsky MD},
  title = {Formal reasoning in adolescence: An alternative view. Adolescence},
  year = {1978}
}

@article{in7,
  author = {Teig N, Scherer R},
  title = {Bringing formal and informal reasoning together—a new era
of assessment? Frontiers in psychology},
  year = {2016}
}

@article{in8,
  author = {Yu F, Zhang H, Tiwari P},
  title = {Natural language reasoning, a survey},
  year = {2023}
}

@article{in9,
  author = {Shao Z, Yu Z, Wang M},
  title = {Prompting large language models with answer heuristics for knowledge-based visual question answering},
  year = {2023}
}


@article{in10,
  author = {Zhu Y, Yuan H, Wang S},
  title = {Large language models for information retrieval: A survey},
  year = {2023}
}


@article{in11,
  author = {Liu Y, Fabbri AR, Liu P},
  title = { On learning to summarize with large language models as references},
  year = {2023}
}

@article{in12,
  author = {Yu F, Zhang H, Tiwari P},
  title = {Natural language reasoning, a survey}
  year = {2023}
}

@article{in13,
  author = {Bommasani R, Hudson DA, Adeli E},
  title = {On the opportunities and risks of foundation models}
  year = {2021}
}

@article{in14,
  author = {Qiao S, Ou Y, Zhang N},
  title = {Reasoning with language model prompting: A survey}
  year = {2022}
}

@article{in15,
  author = {Wang J, Liu Z, Zhao L},
  title = {Review of large vision models and visual prompt engineering}
  year = {2023}
}

@article{in16,
  author = {Li C},
  title = {Large multimodal models: Notes on cvpr 2023 tutorial}
  year = {2023}
}

@article{in17,
  author = {Li C},
  title = {Large multimodal models: Notes on cvpr 2023 tutorial}
  year = {2023}
}

@article{back1,
  author = {Furbach U, H¨olldobler S, Ragni M},
  title = {Cognitive reasoning: A personal view}
  year = {2019}
}

@article{back2,
  author = {Nunes T},
  title = {Logical Reasoning and Learning, Springer US, Boston, MA, pp 2066–2069}
  year = {2012}
}

@article{back3,
  author = {Yu F, Zhang H, Tiwari P},
  title = {Natural language reasoning, a survey}
  year = {2023}
}

@article{back4,
  author = {Evans JSB, Thompson VA},
  title = {Informal reasoning: Theory and method. Canadian Journal of Experimental Psychology/Revue canadienne de psychologie experimentale}
  year = {2004}
}

@article{back5,
  author = {Garcez AS, Lamb LC, Gabbay DM},
  title = {Neural-symbolic cognitive reasoning. Springer Science & Business Media}
  year = {2008}
}

@article{back6,
  author = {Al-Ajlan A},
  title = { The comparison between forward and backward chaining. International Journal of Machine Learning and Computing }
  year = {2015}
}

@article{back7,
  author = {Song X, Shi Y, Chen X},
  title = {Explore multi-step reasoning in video question answering}
  year = {2018}
}

@article{back8,
  author = {EYu F, Zhang H, Tiwari P},
  title = { Natural language reasoning, a survey}
  year = {2023}
}

@article{back9,
  author = {Sowa JF},
  title = {Laws, facts, and contexts: Foundations for multimodal reasoning}
  year = {2003}
}

@article{back10,
  author = {Brown T, Mann B, Ryder N},
  title = { Language models are few-shot learners. Advances in neural information processing systems}
  year = {2020}
}

@article{back11,
  author = {OpenAI},
  title = {Gpt-4 technical report}
  year = {2023}
}

@article{back12,
  author = {Chowdhery A, Narang S, Devlin J},
  title = {Palm: Scaling language modeling with pathways.}
  year = {2022}
}

@article{back13,
  author = {OpenAI},
  title = { Llama: Open and efficient foundation language models}
  year = {2023}
}


@article{back14,
  author = {Touvron H, Martin L, Stone K},
  title = {Open foundation and fine-tuned chat models.}
  year = {2023}
}

@article{back15,
  author = {Dosovitskiy A, Beyer L, Kolesnikov A},
  title = {An image is worth 16x16 words: Transformers for image recognition at scale}
  year = {2021}
}

@article{back16,
  author = {Liu Z, Lin Y, Cao Y},
  title = {Swin transformer: Hierarchical vision transformer using shifted windows}
  year = {2021}
}

@article{back17,
  author = {He K, Chen X, Xie S},
  title = {Masked autoencoders are scalable vision learners. }
  year = {2022}
}

@article{back18,
  author = {Bao H, Dong L, Piao S},
  title = {Beit: Bert pre-training of image transformers}
  year = {2021}
}

@article{back19,
  author = {Chen X, Ding M, Wang X},
  title = {Context autoencoder for self-supervised representation learning.}
  year = {2023}
}

@article{back20,
  author = {Wang L, Huang B, Zhao Z},
  title = {Videomae v2: Scaling video masked autoencoders with dual masking.}
  year = {2023}
}

@article{back21,
  author = {Tong Z, Song Y, Wang J},
  title = {Videomae: Masked autoencoders are dataefficient learners for self-supervised video pre-training}
  year = {2022}
}

@article{back22,
  author = {Yuan L, Chen D, Chen YL},
  title = {Florence: A new foundation model for computer vision}
  year = {2021}
}

@article{back23,
  author = {Ding M, Xiao B, Codella N},
  title = {Davit: Dual attention vision transformers.}
  year = {2022}
}

@article{back24,
  author = {Kirillov A, Mintun E, Ravi N},
  title = { Segment anything}
  year = {2023}
}

@article{back25,
  author = {Zhang J, Zhou Z, Mai G},
  title = {Text2seg: Remote sensing image semantic segmentation via text-guided visual foundation models}
  year = {2023}
}

@article{back26,
  author = {Liu S, Zeng Z, Ren T},
  title = { Grounding dino: Marrying dino with grounded pre-training for open-set object detection}
  year = {2023}
}

@article{back27,
  author = {Radford A, Kim JW, Hallacy C},
  title = { Learning transferable visual models from natural language supervision.}
  year = {2021}
}

@article{back28,
  author = {Jia C, Yang Y, Xia Y},
  title = {Scaling up visual and vision-language representation learning with noisy text supervision}
  year = {2021}
}

@article{back29,
  author = {Huo Y, Zhang M, Liu G},
  title = { Wenlan: Bridging vision and language by large-scale multi-modal pre-training}
  year = {2021}
}

@article{back30,
  author = {Zhou K, Yang J, Loy CC},
  title = {Learning to prompt for vision-language models.}
  year = {2022}
}

@article{back31,
  author = {Tao M, Bao BK, Tang H},
  title = {Galip: Generative adversarial clips for text-toimage synthesis.}
  year = {2023}
}

@article{back32,
  author = {Li Y, Wang H, Duan Y},
  title = {Clip surgery for better explainability with enhancement in open-vocabulary tasks}
  year = {2023}
}

@article{back33,
  author = {He H, Zhang J, Xu M},
  title = { Scalable mask annotation for video text spotting}
  year = {2023}
}

@article{back34,
  author = {Wang T, Zhang J, Fei J},
  title = {Caption anything: Interactive image description with diverse multimodal controls. }
  year = {2023}
}














